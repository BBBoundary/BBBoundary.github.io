---
layout:    post   				    # 使用的布局（不需要改）
title:    【CV-Visual Caption】MARN Model Explanation # 标题 
subtitle:  2019_Wenjie Pei_Memory-Attended Recurrent Network for Video Captioning #副标题
date:      2020-03-10 				# 时间
author:    Culaccino					# 作者
header-img: img/upd_img7.PNG        #这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - CV
    - Visual Caption
    - paper reading
---

**[Paper Download Address](https://arxiv.org/abs/1905.03966?context=cs)**

## Abstract

#### ①Question

传统视频描述的模型几乎都选用了encoder-decoder的框架，而这种框架潜在的缺点之一是，其不能捕捉到当前视频之外的相似内容信息。

#### ②Method&Model

作者提出了一种Memory-Attended Recurrent Network（MARN），声称其可以记录对一个词的全范围的信息记录。

> a memory structureis designed to explore the full-spectrum correspondence between a word and its various similar visual contexts across videos in training data. 

另外，作者还说该模型能够不同于传统模型，其能**显式**地提高caption中单词间的兼容性。（？个人迷惑点）

#### ③Answer

在MSR-VTT、MSVD上经测试达到state-of-the-art。



## Introduction

Video Caption相比Image Caption更有挑战性的地方在于其不仅包含了序列（时间）信息，更是要捕获利用时间信息、以将视频内容作为一个整理来理解。

作者由记忆机制在文件级别的机器翻译中的应用启发，对于传统encoder-decoder框架，作者仍使用了decoder，但在此基础上新增了记忆体，用于存储单词表中的每个单词对应的全局视觉信息（即在训练集中所有与之相关的视觉信息），并将其作为辅助解码器(assistant decoder)。



## Model

#### ①Encoder

本篇paper将2D特征及3D特征进行分开提取，选用ResNet-101（在imagenet上进行预训练过）作为2D特征提取器，ResNeXt-101（在Kinetics上进行预训练过）作为3D特征提取器。

对于给出的视频帧序列$X={x_1,x_2,……x_L}$，ResNet-101将每一帧均提取出对应的$f_i$特征，即对于长度为L的视频帧序列，其得到的2D特征序列为$$F_{2D}=[f_1,f_2,……f_L]\ \ f\in R^d$$。

论文中设置每16帧作为一次时间节点，抽取3D特征。即可得到$$F_{3D}=[v_1,v_2……v_N]\ \ c\in R^c$$。其中N=L/16。

最后，将2D、3D特征均线性变换到m维向量：


$$
f_l'=M_ff_l+b_f\ \ \ \ M_f\in R^{m\*d}\\
v_n'=M_vv_n+b_v\ \ \ \ M_v\in R^{m\*c}\\
$$


#### ②Attention-based Recurrent Decoder

本文将GRU作为decoder（也可以用LSTM替换）,隐藏因子的转移公式为：$h_t=GRU(h_{t-1},e_t,e_{t-1})$。则最终该部分得到的对生成单词概率分布的计算式为：


$$
P_b(w_k)=\frac{exp(W_kh_t+b_k)}{\sum_{i=1}^K(W_ih_t+b_i)}\\
$$


对于t时刻2D、3D特征的编码分别是：


$$
c_{t,2D}=\sum_{i=1}^La_{i,t}f_i'\ \ \ \ \ a_{i,t}=f_{att}(h_{t-1},f_i')\\
c_{t,3D}=\sum_{i=1}^Na_{i,t}'v_i'\ \ \ \ \ a_{i,t}'=f_{att}(h_{t-1},v_i')
$$

一次可以得到最终的信息特征：$c_t=[c_{t,2D};c_{t,3D}]$。并由此对GRU隐藏层因子进行更新：$h_t=GRU(h_{t-1},c_t,e_{t-1})$，由ht最终得到$P_b(w_k)$。

**此处将2D、3D在编码时分开编码可以看作是一种正则化操作，用于降低信息重复编码而导致的过拟合。**



#### ③Attended Memory Decoder

记忆体是以mapping structure进行构造的，对于<w,d>,w为候选单词表中的单词，d包括：a.相关视觉信息(visual context information)，b.词嵌入(word embedding)，c.辅助信息(auxiliary features)。即：


$$
<w_r,d_r>=<w_r,(g_r,e_r,u_r)>.\\
$$


**a.Visual context information**

对于每一个候选词，我们从包含了它的所有视频中得到显著的视觉特征，又为了不造成冗余，我们只取最相关的k个相关特征记录下来：


$$
g_r=\frac{\sum_{i=1}^I\sum_{j=1}^K(\beta_{i,j}f_{i,j}')}{\sum_{i=1}^I\sum_{j=1}^K\beta_{i,j}}+\frac{\sum_{i=1}^I\sum_{j=1}^K(\beta_{i,j}'v_{i,j}')}{\sum_{i=1}^I\sum_{j=1}^K\beta_{i,j}'}\\
$$

其中，I为有关视频的总个数，$\beta_{i,j}$为降序排序后，第i个视频第j相关的特征的权重，$f_{i,j}'、v_{i,j}'$分别为第i个视频第j相关的2D、3D特征。

**b.Word Embedding**

将单词w的语义特征$e_r$也作为记忆模型中的描述内容之一。这一步在Decoder中可以得到（详见GRU或LSTM的实现机制）。

**c.Auxiliary feature**

作者在记忆体描述部分最后加入了相关视频的索引信息，从而有助于视频场景进行大致的聚类，从而辅助解码过程。

~~(讲道理这怎么操作我有点懵逼)~~

————————

作者的该模型将记忆体作为辅助编码器，体现在记忆体也对最后生成单词的分布概率有影响：


$$
P(w_k)=(1-\lambda)P_b(w_k)+\lambda P_m(w_k).\\
$$

其中$P_m(w_k)$即为由记忆体生成的P(w），计算如下：


$$
P_m(w_k)=\frac{exp(q_k)}{\sum_{i=1}^Kexp(q_i)}\\
q_i=v^Ttanh([W_cc_t+W_gg_i]+[W_e'e_{t-1}+W_e'e_i]+W_hh_{t-1}+W_uu_i+b).\\
$$

由此生成的$P(w_k)$即为最终所需用于决策的概率分布。



## Talk

cby从此走上鬼才取名道路的万恶之源，真实地感受到了能用英语描述出来但没办法用中文组织语言的痛苦（可恶啊这加上记忆机制的模型该怎么取名啊），以后我够强了就在自己脑子里安装一个cby专用translator（重点错）

大创涉及到的三篇CV论文已经总结完成了，由于是任务驱动型阅读，Grounding和MARN没有加入实验的部分，所以在标题中也取名为了“Model Explanation”而不是“Summarize”。今后的paper总结就不会落下实验部分辽~
