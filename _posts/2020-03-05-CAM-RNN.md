---
layout:    post   				    # 使用的布局（不需要改）
title:    【CV-Visual Caption】CAM-RNN Model Summarize   # 标题 
subtitle:  2019_BinZhao_CAM-RNN:Co-Attention Model Based RNN for Video Caption   #副标题
date:      2020-03-05 				# 时间
author:    Culaccino					# 作者
header-img: img/upd_img7.PNG 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - CV
    - paper reading
    - Visual Caption
---

##### [Paper Download Address](https://ieeexplore.ieee.xilesou.top/abstract/document/8718523)

## Abstract

#### 1）Question

生成caption时帧中的无关信息及生成的caption(word)对后续信息生成帮助不大（冗余造成干扰），致使模型无法生成有效信息。

#### 2）Method/Model

##### CAM(encoder部分)

①Visual Attention：在图像中定位与caption相关物的位置

	  -  region level ：CNN卷积层对每一帧均提取m个(固定)2D(空间)特征，并对其安全求和，即可得到一帧的特征信息。

		- frame level: CNN卷积层利用region level提取出的2D特征，对取出的几帧提取3D(空间+时间)特征。

②Text Attention:在已生成的caption中定位主体物内容

③Balancing gate：平衡visual&text的内容

##### RNN（decoder部分）:

该模型中选用了LSTM作为解码器，以充分利用历史信息。

#### 3)Answer

已成功在MSVD、Charades、MSR-VTT、MRII-MD上取得不错的成绩。



## Introduction

#### 1）Model Contribution

	 -   两层的visual attention：可以同时定位到关键帧(frame)&关键区域(region)，同时信息结构&帧间平滑度不会被破坏。
	 -   phrase-level text attention mechanism：短语级别的文本注意机制，优化了word-level时非视觉性词汇无法提供所需的前置信息的情况，即phrase-level可以定位到与当前生成词最近的previous caption.
	 -   balancing gate：平衡门，用于生成nonvisual words（a、the、is等），具体方法是在最后单词期望分布的计算公式中在Visual特征前加入控制参数λ，当λ==0时相当于不受visual特征的控制，有利于通过文本特征生成非视觉性词汇，提高语法正确性。

#### 2）Article Structure

SEC II：related work：**a.**Machine Translation   **b.**Image Caption    **c.**Video Caption

SEC III: CAM-RNN

SEC IV：experiments



## Conclusion

在MSVD、Charades、MSR-VTT、MRII-MD等数据集上的得分都较传统方法有了**略微的提升**~~（那提升着实微妙）~~(详见后面Experiments部分~)

该模型可以迁移至image caption、video question-answer等应用中。



## Table&Experiments

#### 1) Results of Baselines

**-Premise**：由VggNet统一提供特征编码内容，控制变量，由此对是三个结构（视觉注意、文本主义、平衡们）进行分别检验是否有效。

**-Results**: 

  - Visual Attention Module: 【对照组：None-RNN、Frame-RNN、Visual-RNN】

    ![Results of VAM](https://github.com/BBBoundary/BBBoundary.github.io/blob/master/img/CV_CAM-img1.png)
  
