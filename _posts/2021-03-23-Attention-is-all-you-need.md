---
layout:    post   				    # 使用的布局（不需要改） 
title:    「CV-Mechanism」Attention is all you need ?    # 标题 
subtitle:  Exploring the truth of Attention in Computer Vision  #副标题
date:      2021-03-23 				# 时间
author:    Culaccino					# 作者
header-img: img/upd_img26.JPG        #这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - Machine Learning
	- CV

---

这已经是笔者第四个学期做CV科研啦~ 这个过程中自己虽然接触到了非常多有趣的模型和机制，却一直浮于其应用表面，没有深入挖掘到算法的设计本质。对于Attention机制，这其实是自我入门visual captioning后一直在接触的一个机制。打自其诞生以来，各种运用其的工作年复一年地刷分，顶会中各种Transformer层出不穷；而自己在学习Attention机制后，也只是简单地将很多观点全盘纳入，认为这就是“模仿了人的注意力机制”，觉得“很有道理”。

正当我在准备这学期的paper（Learning to Generate Grounded Visual Captions without Localization Supervision, ECCV2020）分享时，作者提到了"注意力机制用于确认物体时表现较差，人与网络关于的区域往往是不同的，这使得模型往往缺少解释性"，这使得我不得不重新翻出Attention来看，这才有了很多新收获。故记录下此篇，希望能够一直温故知新。



## 前置知识

#### 弱监督学习&多示例学习（MIL）

在ML领域中，学习任务大致可划分为「监督学习」和「非监督学习」两类。其中，监督学习指数据有label，多用于分类/回归任务，非监督学习的数据无label，多用于聚类任务。目前监督学习已经发展完善，但对每个数据进行标注会使得人力成本过大；而无监督学习因为学习过程太难，导致其发展缓慢。故现在提出了**”弱监督学习“**这一新思路。

弱监督学习可分为：**①不完全监督**（部分实例有label，部分没有） **②不确切监督**（包的概念，几个实例为一个整体包，包有label）  **③不精确监督**（部分label错误）

详细讲解：[https://zhuanlan.zhihu.com/p/81404885](https://zhuanlan.zhihu.com/p/81404885)

**Attention机制主要涉及到不确切监督。**不确切监督是指，训练数据只给出粗粒度标签，几个示例一起组成一个包，我们只知道这个包的标签，Y或N，但不知道每个示例的标签。

实际上，我们可以将所有监督问题都转化为不确切监督问题，即将原来的一个样本拆分为多个小样本，而这些小样本的合集即为一个”包“，原样本的label即为这个包的label——而这也往往更符合实际，例如下图，如果考虑image-level，则判断内容是否为老虎的时候，整幅图的标签就是Y；而神经网络在处理图片时不是整张图一起去看，而是逐个处理由kernel映照到的每个patch（patch-level）。故对于instance9，其label仍为Y，使得整个包(image-level)的label仍能够是Y，而对于instance1、instance2中，是老虎的label就是N了。

<img src="https://tva1.sinaimg.cn/large/008eGmZEly1gouu17ox6ej30qi0dk75q.jpg" style="zoom:80%;" />

为了解决不确切监督问题，我们可以使用**多示例学习**。一个包中若存在一个正例，则包的标签为正；若一个包中全部都为负样例，则包的标签为负；当然，也可以允许包中实例无标签的情况。多示例学习就是模型对已有label的包及其包含的多个实例进行分析，然后去预测其他包的label。

**Attention从这个角度来解释的话，可以理解为就是利用多示例学习求解不确切监督问题的迭代过程。**详细可以见下面的分析。



#### Bottom-up & Top-down

可简单理解为：下层为感知，即对外界的感受输入，是有侧重、较不全面的；上层：认知，即对事物整体的把握学习，是全面的、可量化的。

**Bottom-up（自下而上）：感知指导认知，产生决策和行为**（Data-driving approach stating that perception directs cognition）。自下而上的处理可理解为将感应器结果作为输入（即激励），即自下而上可以说是”数据驱动“。例如，当听见文字声音时，大脑进行处理，得到”是蚊子的信息“，并产生动作去打蚊子。在这个过程中，与文字有关的听觉信息（激励）都听过听神经传递到大脑，然后大脑分析后得到认知信息，这些信息的传递是单方向的。神经网络在处理图片时，也是通过根据输入的激励得到下一步图像上的处理动作。

**Top-down（自上而下）：认知帮助完善感知**（Behavior is influenced by Conceptual Data）。自上而下即通过全局信息来得到当前的图像关注点。利用全局信息，即使用了上下文信息。例如，当看到一张字迹潦草难以辨认的手写文本的时候，我们可以利用整个文本（先验知识）来辅助理解其中的含义，如此一来一些原本无法单独辨认的文字也可以理解了。人脑除了Bottom-up，还有复杂的Top-down机制，即从高级脑区，自上而下调节低层皮层，这会受到记忆因素、注意力因素等的影响。

**Attention机制即是希望能够模拟部分人脑的Top-down机制，还原其中的注意力的影响。**人在看到一个全新的物体时并不需要像机器一样重新训练，**人脑对于低级特征的处理是自动的，故可以自动去调用以前的记忆和相似的物体训练的结果**，故人脑可以通过少量的例子学会一个新物体。 当然，注意力只是人脑机制中的其中一个因素，且当前对人脑的Top-down机制研究仍较基础，同时也有计算机算力/存储能力的影响，很多功能神经网络都还不能模拟出来。



## Attention-逻辑抽象

从Encoder-Decoder的具体实现框架中剥离出，对Attention进行逻辑抽象后，其本质可以被描述为：**一个查询（Query）到一系列键值对（Key-Value）的映射。**

<img src="https://tva1.sinaimg.cn/large/008eGmZEly1gouyknxgzuj30of0lc11z.jpg" style="zoom:67%;" />

**(Soft-)Attention的输出为获得输入的权重分布，最后加权求和得到的输出结果，即为”加入注意力因素后的所需特征“。**具体的计算过程为：

1. 将每个Query与Key进行相似度计算得到权重：Similarity(Query, Key)。常用的相似度函数有点积、余弦相似性、感知机等。
2. 使用softmax函数对这些权重进行归一化：Softmax(Similarity(Query,Key)) ——> Similarity*(Query, Key)。
3. 将权重与键值Value加权求和得到最后的Attention结果。

**感性理解：**以通过输入关键词搜索文章，并阅读相关部分为例。

- Query：用户的需求，键入的关键词。
- Key：从整篇文章中提取出的所有关键词。
- Value：文章的完整内容。
- Similarity(Query, Key)：Query与Key的匹配程度，得到每个Key的Attention score（权值分布）。对于匹配度高的，权值则高。
- 用该分布与键值对中的Value作加权求和，得到的即为用户需求的文章中的匹配部分，即模拟筛选出“用户最需要最关注的部分”。

目前以存在多种Attention，而所有Attention的本质都可以以这种Q/K-V模型来解释。例如对于self-attention，其Q和K则相同，即自己询问自己，自己关注自己；又或者在VQA领域中，对于这种外界提出问题的情况，不同的问题会产生不同的attention，则可以使用multi-head attention，即Query由外界输入，由其引导产生attention。

而在Q/K-V模型中，Query/Key/Value都由输入值乘上一个权重矩阵得到:

<img src="https://tva1.sinaimg.cn/large/008eGmZEly1gouz8dasr6j30bs07e77d.jpg" style="zoom:60%;" />

这三个权重矩阵在实际中是怎么学习得到的，输入的α向量究竟代表什么，为什么由其产生的Q/K-V能够说是模拟出了Attention机制……这些问题在逻辑抽象中是无法得到解释的，故让我们回到当前一些经典的融入了Attention机制的具体模型中去寻找答案。



## Attention-具体实现

#### 机制核心思想

从注意力的实现原理上来讲，CV中常用到回溯性注意力（如CAM、GradCAM等Top-down注意力解释方法）。先通过分类器给出分类结论，后回溯对分类做出强贡献的区域，CAM等方法通过对特征图加权求和的方式将关键区域高亮出来。不同方法所使用的权重不同，但都能揭示模型下结论时关注的是哪些区域。以下解释以周博磊老师的CAM模型[1]为例进行分析。

![](https://tva1.sinaimg.cn/large/008eGmZEly1gov239vxk8j31kw0kldhh.jpg)

CAM模型的核心思想可概括为：


$$
M_c(x,y)=\Sigma_kw_k^cf_k(x,y)\\
S_c=\Sigma_kw_k^c\Sigma_{x,y}f_k(x,y)=\Sigma_{x,y}M_c(x,y)
$$


其中，w_{k,c}表示CNN分类器中最后的FC层的第k个输入神经元对第c个类别的贡献，f_k(x,y)表示第k嗝特征在位置（x,y）上的值，Mc(x,y)表示第c个类别对应的CAM在（x,y）上的值，Sc代表网络预测的在整个图片上第c个类的分数。**其中Mc(x,y)即表示了当前位置对于网络预测图像第c个类别的贡献，即模型对于该位置的重视程度/注意力。**

此外，对于w{k,c}，类似的工作Grad-CAM[2]的区别主要在将CAM中前向传播获取得到的w{k,c}替换为反向传播的梯度。也可以认为，以CAM为例的前向传播是Bottom-up注意力，以Grad-CAM为例的反向传播是Top-down注意力。

从注意力学习过程讲，我认为可以将其理解为是利用MIL求解不确切监督学习的迭代过程。MIL为不确切监督学习提出了一种样本分包学习的流程。在这里，我们可以认为输入的一张图就是一个包，神经网络在处理一张图片时，根据其kernel不断主动分出各个小patch，每个小patch就是这个包中的实例，而包含目标的patch是真正的正样本。

训练的回溯过程即是MIL为各个patch赋予伪标签的过程：CNN分类器会根据CAM注意力得到的patch score和预测效果，寻找包中的强响应样本，并为其赋予伪的正标签，然后通过鸡生蛋蛋生鸡不断迭代优化；而在使用模型时，最后的FC层提供的w{k,c}即是CNN分类器学习后、为CAM提供的patch级的伪标签（并不是严格意义上的0/1离散标签，而是通过权值大小这一连续值来表示其重要性）。

#### 存在的问题

<img src="https://tva1.sinaimg.cn/large/008eGmZEly1gov33i8glvj30ym0eqdn9.jpg" style="zoom:50%;" />

从注意力形成过程也看得出来，伪标签（如上图）从原理上来说就不是绝对正确的机制，是同时包含了无监督和有监督学习的特质的（先无后有）。**更何况在Visual Caption任务中，原始数据集是完全没有label的（image-level的都没有），就连最开始的强响应正样本也是根据训练时引入的伪标签，所以完全有可能一开始给错了label后面就一直错下去。**这也便是Attention最后在高亮标记时会出现的两个问题：

- **正确性问题/误检问题**，即注意力强调的位置不符合人预期的结果。例如，船-水的问题。由于船和水这两个语义在数据集中又高度的统计相关性，根据这些数据集训练出来的模型可能会完全混淆船和水两个概念。但是，船和水经常一起出现这一特征有时也是需要被使用到的，故而不能简单通过修改数据分布（增加没有船的水、没有水的船）来解决问题。
- **质量问题/漏检问题**，即在位置正确时形状、数量描述不佳的现象。比如在处理大目标、多目标定位时往往输出不完整、只能高亮出目标的部分区域。这是因为模型用“分类器”，即**用解决分类问题的思路来尝试解决定位任务**，任务目标的不一致性导致的结果就从中可以体现了。我很喜欢这个解释：“就像光路永远最短一样，优化的贪心性会驱使模型在解决完分类任务之后就不再充分探索剩余信息，这导致分类模型看到目标最有区分力的区域就直接下分类结论了”。针对这点，有一些列工作提出了不额外引入监督信息的解决方法，即每次将图上CAM高亮区域挖掉了以后再喂给下一级模型、通过这种多模型级联学习来强化注意力完整性，将目标区域更完整地挖掘出来。



#### 思考

无论相关方法具体如何实现，至少对于Visual captioning领域，Attention中所有的参数都是根据先验（训练数据）得到的，虽然被称作“能够使模型attended“，但实际上无法做到”dynamicly attended“。当然，对比于VQA等对需求变换要求很高的课题而言，captioning还是相对来说比较静态的——但这份静态，更多是出于设计者的直觉。我们并不知道对于真正需要用到captioning技术的受众人群而言，他们想要得到的是什么描述结果。我想、可能在前后描述内容上的连贯性、由内容产生的注意力，可能是一个比较有用的研究工作，至少会更有温度一些。



——

**参考文献**：

[1] Zhou, Bolei, et al. "Learning deep features for discriminative localization." In CVPR 2016.

[2] Selvaraju, Ramprasaath R., et al. "Grad-cam: Visual explanations from deep networks via gradient-based localization." In ICCV 2017.

